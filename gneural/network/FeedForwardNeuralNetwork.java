package gneural.network;import java.util.ArrayList;import java.util.Random;import gneural.function.ActivationFunction;import gneural.function.Sigmoid;public class FeedForwardNeuralNetwork {	private InputLayer inputs;	private OutputLayer outputs;	private double[][] inputWeights;	private ArrayList<NeuronLayer> layers = new ArrayList<>();	private ArrayList<double[][]> weights = new ArrayList<>();	private ActivationFunction function = new Sigmoid();	// makes sure the structure can't be finalized multiple times	private boolean isFinalized = false;	public FeedForwardNeuralNetwork() {}	public FeedForwardNeuralNetwork(int... layerSizes) {		setInputNeurons(layerSizes[0]);		setInputWeights(randomWeightsNoBias(layerSizes[0], layerSizes[1]));		for (int i = 1; i < layerSizes.length - 1; i++) {			addLayer(new NeuronLayer(layerSizes[i]), randomWeightsNoBias(layerSizes[i], layerSizes[i + 1]));		}		setOutputNeurons(layerSizes[layerSizes.length - 1]);		finalize();	}	public void setInputNeurons(int inputNeurons) {		inputs = new InputLayer(inputNeurons);	}	public void setOutputNeurons(int outputNeurons) {		outputs = new OutputLayer(outputNeurons);		outputs.setActivationFunction(function);	}	public void setActivationFunction(ActivationFunction function) {		this.function = function;		// check if there are already any neural layers, if so, set their		// activation functions		if (outputs != null)			outputs.setActivationFunction(function);		for (int i = 0; i < layers.size(); i++) {			layers.get(i).setActivationFunction(function);		}	}	public void addLayer(NeuronLayer n, double[][] weights) {		// set the layer's activation function		n.setActivationFunction(function);		// add the layer to layers and the weights to weights		layers.add(n);		this.weights.add(weights);	}	public void setInputWeights(double[][] weights) {		inputWeights = weights;	}	public void finalize() {		// make sure the structure hasn't already been finalized		if (isFinalized)			throw new RuntimeException("Can't finalize an already finalized neural network structure!");		// set the structure as finalized		isFinalized = true;		// link each layer to the next		inputs.linkToLayer(layers.get(0), inputWeights);		for (int i = 0; i < layers.size() - 1; i++) {			layers.get(i).linkToLayer(layers.get(i + 1), weights.get(i));		}		layers.get(layers.size() - 1).linkToLayer(outputs, weights.get(weights.size() - 1));	}	public double[] compute(double[] inputData) {		inputs.takeInputs(inputData);		for (int i = 0; i < layers.size(); i++) {			layers.get(i).activateLayer();		}		return outputs.getOutputs();	}	/**	 * performs the backpropagation algorithm on the neural network	 * 	 * @param inputData	 *            data for the network to take in	 * @param expectedOutputs	 *            the expected output of the network	 * @param learningRate	 *            how fast the network will attempt to learn the data	 * @return percent correlation of the data to the expected results before	 *         the backpropagation was performed	 * @since May 1, 2016	 */	public double backpropagate(double[] inputData, double[] expectedOutputs, double learningRate) {		// run the input data through the network and get the outputs		double[] outputs = compute(inputData);		// go through the outputs and compute the error for each of the values		double[] delta = new double[outputs.length];		for (int i = 0; i < delta.length; i++) {			delta[i] = (outputs[i] - expectedOutputs[i])					* function.derivate(this.outputs.getNeuron(i).getPreviousInput());		}		// loop through each layer and propagate the correction back throughout		// the network		for (int i = layers.size() - 1; i >= 0; i--) {			NeuronLayer layer = layers.get(i);			Neuron[] neurons = layer.neurons;			double[] nextDelta = new double[neurons.length];			for (int j = 0; j < neurons.length; j++) {				nextDelta[j] = neurons[j].backpropagate(learningRate, delta);			}						delta = nextDelta;		}		Neuron[] neurons = this.inputs.neurons;		for (int j = 0; j < neurons.length; j++) {			neurons[j].backpropagate(learningRate, delta);		}		return 0;	}	public String toCSVString() {		StringBuilder results = new StringBuilder();		// set the activation function for the network		if (function.getClass().equals(Sigmoid.class))			results.append(0 + ",");		else			results.append(1 + ",");		// loop through all of the neurons and get their lengths		results.append(inputs.neurons.length + ",");		for (int i = 0; i < layers.size(); i++) {			results.append(layers.get(i).neurons.length + ",");		}		results.append(outputs.neurons.length + ",0,");		// loop through and add all of the weights		for (int i = 0; i <= inputs.size(); i++) {			Neuron n = inputs.getNeuron(i);			for (int j = 0; j < n.links.size(); j++) {				results.append(n.links.get(j).getSecond() + ",");			}		}		for (int i = 0; i < layers.size(); i++) {			NeuronLayer layer = layers.get(i);			for (int j = 0; j < layer.weightedSize(); j++) {				Neuron n = layer.getNeuron(j);				for (int k = 0; k < n.links.size(); k++) {					results.append(n.links.get(k).getSecond() + ",");				}			}		}		// remove the final comma		results.deleteCharAt(results.length() - 1);		// return the results		return results.toString();	}	private double[][] randomWeightsNoBias(int width, int height) {		double[][] results = new double[width + 1][height];		Random r = new Random();		for (int i = 0; i < width; i++) {			for (int j = 0; j < height; j++) {				results[i][j] = r.nextDouble();			}		}		return results;	}}